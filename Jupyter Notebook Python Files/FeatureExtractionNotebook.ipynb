{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This is an example of performing Video Activity Recognition using LSTM\n",
    "Modified from \"Hands-on Computer Vision with TensorFlow 2\" by B. Planche and E. Andres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/subhi/Downloads/demo/Data1/**/*.mp4\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = '/Users/subhi/Downloads/demo/Data1'\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, '**','*.mp4')\n",
    "\n",
    "#this specifies the sequence length will process by LSTM\n",
    "SEQUENCE_LENGTH = 40\n",
    "BATCH_SIZE = 16\n",
    "print(VIDEOS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3:  sample the video --do not process every frame\n",
    "PART 1: define function frame_generator(), that creates Sequence_length samples by taking every Kth sample were K= num_frames_in_video / SEQUENCE LENGTH.\n",
    "\n",
    "PART 2: you load the DataSet and specify the output will be frames of size 224x224 x3(rgb) AND you create batches of size 16 together at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART1\n",
    "def frame_generator():\n",
    "    video_paths = tf.io.gfile.glob(VIDEOS_PATH)\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "        \n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        \n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "                \n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                frame = frame[:, :, ::-1]\n",
    "                img = tf.image.resize(frame, (224, 224))\n",
    "                img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "                max_images -= 1\n",
    "                yield img, video_path\n",
    "                \n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART2\n",
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "                                         output_types=(tf.float32, tf.string),\n",
    "                                         output_shapes=((224, 224, 3), ()))\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((None, 224, 224, 3), (None,)), types: (tf.float32, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: \n",
    "For Feature Extraction we are going to use a existing CNN model called MobileNet which is built into TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "x = mobilenet_v2.output\n",
    "\n",
    "# We add Average Pooling to transform the feature map from\n",
    "# 8 * 8 * 1280 to 1 x 1280, as we don't need spatial information\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: \n",
    "Extract Features using our MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't run this for if done with feature extraction \n",
    "current_path = None\n",
    "all_features = []\n",
    "\n",
    "#cycle through the dataset and visit each image, note the tdqm is a progress bar\n",
    "#that updates each time a new iteration is called \n",
    "#call feature_extraction_model above (Inception v3) for the image to extract the features\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    batch_features = feature_extraction_model(img)\n",
    "    #reshape the tensor \n",
    "    batch_features = tf.reshape(batch_features, \n",
    "                              (batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        if path != current_path and current_path is not None:\n",
    "            output_path = current_path.decode().replace('.mp4', '.npy')\n",
    "            np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelBinarizer(LabelBinarizer):\n",
    "    def transform(self, y):\n",
    "        Y = super().transform(y)\n",
    "        if self.y_type_ == 'binary':\n",
    "            return np.hstack((Y, 1-Y))\n",
    "        else:\n",
    "            return Y\n",
    "    def inverse_transform(self, Y, threshold=None):\n",
    "        if self.y_type_ == 'binary':\n",
    "            return super().inverse_transform(Y[:, 0], threshold)\n",
    "        else:\n",
    "            return super().inverse_transform(Y, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS = ['Stairs','Doors']\n",
    "encoder = MyLabelBinarizer()\n",
    "encoder.fit(LABELS)\n",
    "#print(encoder.classes_)\n",
    "#print(encoder.transform(['Doors', 'Stairs']))\n",
    "\n",
    "#t= encoder.transform(['Doors', 'Stairs', 'Stairs'])\n",
    "#print(t)\n",
    "#print(encoder.inverse_transform(t))\n",
    "#print(\"length of labrels \" + str(len(LABELS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABELS = ['Doors','Stairs','Other']\n",
    "# encoder = LabelBinarizer()\n",
    "# encoder.fit(LABELS)\n",
    "# print(encoder.classes_)\n",
    "# #print(encoder.transform(['Doors','stairs']))\n",
    "# print(\"length of labels is \" + str(len(LABELS)))\n",
    "# print(str(encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: \n",
    "    Create the LSTM model:    1) Masking layer  2) LSTM layer with 512 cells, dropout 0.5, recurrent_dropout of 0.5  \n",
    " 3) a fully connected relu activation layer with 256 outputs,  4) a droupout layer 0.5  5) a final decision fully connected layer of putput length of labels  (which is the number of classes) with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup a keras Sequential model with 1) Masking layer  2) LSTM layer with 512 cells, dropout 0.5, recurrent_dropout of 0.5  \n",
    "# 3) a fully connected relu activation layer with 256 outputs,  4) a droupout layer 5) a final decision fully connected layer of length of labels\n",
    "# (which is the number of classes) with softmax activation.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.),\n",
    "    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "    #tf.keras.layers.Dense(len(LABELS), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: \n",
    "Setup for the model the Loss function, the Optimizer function, and any metrics want to compute in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy', 'top_k_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: \n",
    "Setup  the training and test list which are lists of the training filenames.   Note you will need to change the location of these files to point to your location.  Define a function make_generator that returns a generator which will randomly shuffle a file list (either training or testing that will be passed later) and then changes the file extension of the avi files listed in the list to .npy which is our features for that avi video which were calcluated in step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '/Users/subhi/Downloads/demo/Data1/trainlist.txt'\n",
    "test_file = '/Users/subhi/Downloads/demo/Data1/testlist.txt'\n",
    "#test_file = 'C:/Grewe/Classes/CS663/Mat/LSTM/data/testlist01.txt'\n",
    "#train_file = 'C:/Grewe/Classes/CS663/Mat/LSTM/data/trainlist01.txt'\n",
    "\n",
    "with open(test_file) as f:\n",
    "    test_list = [row.strip() for row in list(f)]\n",
    "\n",
    "with open(train_file) as f:\n",
    "     train_list = [row.strip() for row in list(f)]\n",
    "     #train_list=[row.split(' ')[0] for row in train_list]\n",
    "     train_list = [row.split(' ')[0] for row in train_list]\n",
    "#print(train_list)\n",
    "def make_generator(file_list):\n",
    "    def generator():\n",
    "        np.random.shuffle(file_list)\n",
    "        for path in file_list:\n",
    "            full_path = os.path.join(BASE_PATH, path).replace('.mp4', '.npy')\n",
    "            \n",
    "            label = os.path.basename(os.path.dirname(path))\n",
    "            features = np.load(full_path)\n",
    "            \n",
    "            \n",
    "            padded_sequence = np.zeros((SEQUENCE_LENGTH, 1280))\n",
    "            padded_sequence[0:len(features)] = np.array(features)\n",
    "            \n",
    "            transformed_label = encoder.transform([label])\n",
    "            \n",
    "            yield padded_sequence, transformed_label[0]\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doors/1.mp4', 'Doors/10.mp4', 'Doors/100.mp4', 'Doors/101.mp4', 'Doors/102.mp4', 'Doors/103.mp4', 'Doors/104.mp4', 'Doors/105.mp4', 'Doors/106.mp4', 'Doors/109.mp4', 'Doors/11.mp4', 'Doors/110.mp4', 'Doors/111.mp4', 'Doors/112.mp4', 'Doors/113.mp4', 'Doors/114.mp4', 'Doors/116.mp4', 'Doors/117.mp4', 'Doors/118.mp4', 'Doors/119.mp4', 'Doors/12.mp4', 'Doors/120.mp4', 'Doors/121.mp4', 'Doors/122.mp4', 'Doors/124.mp4', 'Doors/125.mp4', 'Doors/126.mp4', 'Doors/128.mp4', 'Doors/129.mp4', 'Doors/130.mp4', 'Doors/131.mp4', 'Doors/132.mp4', 'Doors/133.mp4', 'Doors/135.mp4', 'Doors/136.mp4', 'Doors/137.mp4', 'Doors/14.mp4', 'Doors/140.mp4', 'Doors/141.mp4', 'Doors/142.mp4', 'Doors/143.mp4', 'Doors/144.mp4', 'Doors/145.mp4', 'Doors/146.mp4', 'Doors/147.mp4', 'Doors/148.mp4', 'Doors/15.mp4', 'Doors/153.mp4', 'Doors/154.mp4', 'Doors/155.mp4', 'Doors/156.mp4', 'Doors/157.mp4', 'Doors/159.mp4', 'Doors/16.mp4', 'Doors/160.mp4', 'Doors/161.mp4', 'Doors/162.mp4', 'Doors/163.mp4', 'Doors/165.mp4', 'Doors/166.mp4', 'Doors/167.mp4', 'Doors/168.mp4', 'Doors/169.mp4', 'Doors/17.mp4', 'Doors/170.mp4', 'Doors/172.mp4', 'Doors/173.mp4', 'Doors/174.mp4', 'Doors/176.mp4', 'Doors/177.mp4', 'Doors/178.mp4', 'Doors/179.mp4', 'Doors/181.mp4', 'Doors/183.mp4', 'Doors/185.mp4', 'Doors/186.mp4', 'Doors/187.mp4', 'Doors/188.mp4', 'Doors/19.mp4', 'Doors/190.mp4', 'Doors/192.mp4', 'Doors/194.mp4', 'Doors/195.mp4', 'Doors/196.mp4', 'Doors/199.mp4', 'Doors/2.mp4', 'Doors/200.mp4', 'Doors/201.mp4', 'Doors/202.mp4', 'Doors/204.mp4', 'Doors/205.mp4', 'Doors/206.mp4', 'Doors/207.mp4', 'Doors/208.mp4', 'Doors/209.mp4', 'Doors/21.mp4', 'Doors/211.mp4', 'Doors/213.mp4', 'Doors/214.mp4', 'Doors/216.mp4', 'Doors/217.mp4', 'Doors/218.mp4', 'Doors/219.mp4', 'Doors/22.mp4', 'Doors/220.mp4', 'Doors/222.mp4', 'Doors/223.mp4', 'Doors/225.mp4', 'Doors/226.mp4', 'Doors/227.mp4', 'Doors/228.mp4', 'Doors/229.mp4', 'Doors/23.mp4', 'Doors/230.mp4', 'Doors/232.mp4', 'Doors/233.mp4', 'Doors/234.mp4', 'Doors/235.mp4', 'Doors/236.mp4', 'Doors/237.mp4', 'Doors/238.mp4', 'Doors/239.mp4', 'Doors/24.mp4', 'Doors/240.mp4', 'Doors/241.mp4', 'Doors/242.mp4', 'Doors/243.mp4', 'Doors/244.mp4', 'Doors/246.mp4', 'Doors/247.mp4', 'Doors/248.mp4', 'Doors/249.mp4', 'Doors/25.mp4', 'Doors/250.mp4', 'Doors/251.mp4', 'Doors/252.mp4', 'Doors/253.mp4', 'Doors/255.mp4', 'Doors/257.mp4', 'Doors/258.mp4', 'Doors/259.mp4', 'Doors/26.mp4', 'Doors/260.mp4', 'Doors/262.mp4', 'Doors/263.mp4', 'Doors/265.mp4', 'Doors/266.mp4', 'Doors/269.mp4', 'Doors/27.mp4', 'Doors/270.mp4', 'Doors/273.mp4', 'Doors/275.mp4', 'Doors/276.mp4', 'Doors/277.mp4', 'Doors/279.mp4', 'Doors/28.mp4', 'Doors/280.mp4', 'Doors/282.mp4', 'Doors/283.mp4', 'Doors/287.mp4', 'Doors/288.mp4', 'Doors/289.mp4', 'Doors/290.mp4', 'Doors/291.mp4', 'Doors/293.mp4', 'Doors/294.mp4', 'Doors/295.mp4', 'Doors/296.mp4', 'Doors/297.mp4', 'Doors/298.mp4', 'Doors/299.mp4', 'Doors/3.mp4', 'Doors/30.mp4', 'Doors/300.mp4', 'Doors/301.mp4', 'Doors/303.mp4', 'Doors/306.mp4', 'Doors/307.mp4', 'Doors/308.mp4', 'Doors/309.mp4', 'Doors/31.mp4', 'Doors/310.mp4', 'Doors/311.mp4', 'Doors/312.mp4', 'Doors/315.mp4', 'Doors/316.mp4', 'Doors/317.mp4', 'Doors/318.mp4', 'Doors/319.mp4', 'Doors/32.mp4', 'Doors/320.mp4', 'Doors/321.mp4', 'Doors/322.mp4', 'Doors/323.mp4', 'Doors/324.mp4', 'Doors/325.mp4', 'Doors/327.mp4', 'Doors/328.mp4', 'Doors/329.mp4', 'Doors/33.mp4', 'Doors/331.mp4', 'Doors/332.mp4', 'Doors/334.mp4', 'Doors/337.mp4', 'Doors/338.mp4', 'Doors/34.mp4', 'Doors/340.mp4', 'Doors/341.mp4', 'Doors/342.mp4', 'Doors/345.mp4', 'Doors/346.mp4', 'Doors/347.mp4', 'Doors/348.mp4', 'Doors/349.mp4', 'Doors/35.mp4', 'Doors/350.mp4', 'Doors/352.mp4', 'Doors/354.mp4', 'Doors/356.mp4', 'Doors/359.mp4', 'Doors/360.mp4', 'Doors/361.mp4', 'Doors/362.mp4', 'Doors/363.mp4', 'Doors/364.mp4', 'Doors/365.mp4', 'Doors/366.mp4', 'Doors/367.mp4', 'Doors/368.mp4', 'Doors/369.mp4', 'Doors/37.mp4', 'Doors/370.mp4', 'Doors/371.mp4', 'Doors/372.mp4', 'Doors/373.mp4', 'Doors/374.mp4', 'Doors/376.mp4', 'Doors/377.mp4', 'Doors/378.mp4', 'Doors/38.mp4', 'Doors/380.mp4', 'Doors/381.mp4', 'Doors/382.mp4', 'Doors/383.mp4', 'Doors/384.mp4', 'Doors/385.mp4', 'Doors/387.mp4', 'Doors/388.mp4', 'Doors/39.mp4', 'Doors/390.mp4', 'Doors/393.mp4', 'Doors/394.mp4', 'Doors/395.mp4', 'Doors/396.mp4', 'Doors/397.mp4', 'Doors/398.mp4', 'Doors/399.mp4', 'Doors/4.mp4', 'Doors/40.mp4', 'Doors/400.mp4', 'Doors/41.mp4', 'Doors/42.mp4', 'Doors/43.mp4', 'Doors/44.mp4', 'Doors/45.mp4', 'Doors/46.mp4', 'Doors/48.mp4', 'Doors/49.mp4', 'Doors/50.mp4', 'Doors/52.mp4', 'Doors/55.mp4', 'Doors/56.mp4', 'Doors/57.mp4', 'Doors/58.mp4', 'Doors/59.mp4', 'Doors/6.mp4', 'Doors/61.mp4', 'Doors/65.mp4', 'Doors/66.mp4', 'Doors/67.mp4', 'Doors/68.mp4', 'Doors/69.mp4', 'Doors/70.mp4', 'Doors/72.mp4', 'Doors/73.mp4', 'Doors/75.mp4', 'Doors/77.mp4', 'Doors/78.mp4', 'Doors/79.mp4', 'Doors/8.mp4', 'Doors/80.mp4', 'Doors/82.mp4', 'Doors/84.mp4', 'Doors/85.mp4', 'Doors/87.mp4', 'Doors/88.mp4', 'Doors/89.mp4', 'Doors/9.mp4', 'Doors/90.mp4', 'Doors/91.mp4', 'Doors/94.mp4', 'Doors/95.mp4', 'Doors/96.mp4', 'Doors/97.mp4', 'Doors/98.mp4', 'Doors/99.mp4', 'Stairs/10.mp4', 'Stairs/100.mp4', 'Stairs/101.mp4', 'Stairs/102.mp4', 'Stairs/103.mp4', 'Stairs/104.mp4', 'Stairs/105.mp4', 'Stairs/106.mp4', 'Stairs/107.mp4', 'Stairs/11.mp4', 'Stairs/110.mp4', 'Stairs/111.mp4', 'Stairs/112.mp4', 'Stairs/113.mp4', 'Stairs/115.mp4', 'Stairs/116.mp4', 'Stairs/117.mp4', 'Stairs/118.mp4', 'Stairs/12.mp4', 'Stairs/120.mp4', 'Stairs/122.mp4', 'Stairs/123.mp4', 'Stairs/124.mp4', 'Stairs/125.mp4', 'Stairs/126.mp4', 'Stairs/127.mp4', 'Stairs/128.mp4', 'Stairs/129.mp4', 'Stairs/13.mp4', 'Stairs/131.mp4', 'Stairs/132.mp4', 'Stairs/133.mp4', 'Stairs/134.mp4', 'Stairs/136.mp4', 'Stairs/137.mp4', 'Stairs/138.mp4', 'Stairs/139.mp4', 'Stairs/14.mp4', 'Stairs/140.mp4', 'Stairs/141.mp4', 'Stairs/142.mp4', 'Stairs/143.mp4', 'Stairs/144.mp4', 'Stairs/146.mp4', 'Stairs/147.mp4', 'Stairs/148.mp4', 'Stairs/149.mp4', 'Stairs/15.mp4', 'Stairs/150.mp4', 'Stairs/153.mp4', 'Stairs/157.mp4', 'Stairs/159.mp4', 'Stairs/160.mp4', 'Stairs/161.mp4', 'Stairs/162.mp4', 'Stairs/163.mp4', 'Stairs/165.mp4', 'Stairs/166.mp4', 'Stairs/169.mp4', 'Stairs/17.mp4', 'Stairs/170.mp4', 'Stairs/171.mp4', 'Stairs/172.mp4', 'Stairs/173.mp4', 'Stairs/174.mp4', 'Stairs/175.mp4', 'Stairs/176.mp4', 'Stairs/177.mp4', 'Stairs/178.mp4', 'Stairs/179.mp4', 'Stairs/18.mp4', 'Stairs/180.mp4', 'Stairs/182.mp4', 'Stairs/183.mp4', 'Stairs/184.mp4', 'Stairs/185.mp4', 'Stairs/186.mp4', 'Stairs/188.mp4', 'Stairs/189.mp4', 'Stairs/19.mp4', 'Stairs/190.mp4', 'Stairs/191.mp4', 'Stairs/193.mp4', 'Stairs/194.mp4', 'Stairs/195.mp4', 'Stairs/198.mp4', 'Stairs/199.mp4', 'Stairs/20.mp4', 'Stairs/200.mp4', 'Stairs/203.mp4', 'Stairs/206.mp4', 'Stairs/207.mp4', 'Stairs/209.mp4', 'Stairs/21.mp4', 'Stairs/210.mp4', 'Stairs/212.mp4', 'Stairs/213.mp4', 'Stairs/214.mp4', 'Stairs/215.mp4', 'Stairs/216.mp4', 'Stairs/217.mp4', 'Stairs/218.mp4', 'Stairs/219.mp4', 'Stairs/22.mp4', 'Stairs/220.mp4', 'Stairs/221.mp4', 'Stairs/224.mp4', 'Stairs/227.mp4', 'Stairs/229.mp4', 'Stairs/230.mp4', 'Stairs/231.mp4', 'Stairs/232.mp4', 'Stairs/233.mp4', 'Stairs/234.mp4', 'Stairs/236.mp4', 'Stairs/24.mp4', 'Stairs/240.mp4', 'Stairs/242.mp4', 'Stairs/244.mp4', 'Stairs/245.mp4', 'Stairs/246.mp4', 'Stairs/247.mp4', 'Stairs/248.mp4', 'Stairs/249.mp4', 'Stairs/25.mp4', 'Stairs/250.mp4', 'Stairs/252.mp4', 'Stairs/253.mp4', 'Stairs/254.mp4', 'Stairs/255.mp4', 'Stairs/256.mp4', 'Stairs/257.mp4', 'Stairs/259.mp4', 'Stairs/26.mp4', 'Stairs/260.mp4', 'Stairs/261.mp4', 'Stairs/262.mp4', 'Stairs/264.mp4', 'Stairs/265.mp4', 'Stairs/266.mp4', 'Stairs/267.mp4', 'Stairs/268.mp4', 'Stairs/269.mp4', 'Stairs/27.mp4', 'Stairs/270.mp4', 'Stairs/272.mp4', 'Stairs/273.mp4', 'Stairs/274.mp4', 'Stairs/276.mp4', 'Stairs/277.mp4', 'Stairs/278.mp4', 'Stairs/279.mp4', 'Stairs/28.mp4', 'Stairs/280.mp4', 'Stairs/281.mp4', 'Stairs/282.mp4', 'Stairs/283.mp4', 'Stairs/285.mp4', 'Stairs/286.mp4', 'Stairs/288.mp4', 'Stairs/289.mp4', 'Stairs/29.mp4', 'Stairs/290.mp4', 'Stairs/291.mp4', 'Stairs/292.mp4', 'Stairs/293.mp4', 'Stairs/295.mp4', 'Stairs/297.mp4', 'Stairs/299.mp4', 'Stairs/3.mp4', 'Stairs/30.mp4', 'Stairs/300.mp4', 'Stairs/301.mp4', 'Stairs/303.mp4', 'Stairs/304.mp4', 'Stairs/305.mp4', 'Stairs/306.mp4', 'Stairs/307.mp4', 'Stairs/308.mp4', 'Stairs/31.mp4', 'Stairs/311.mp4', 'Stairs/312.mp4', 'Stairs/313.mp4', 'Stairs/314.mp4', 'Stairs/315.mp4', 'Stairs/317.mp4', 'Stairs/318.mp4', 'Stairs/319.mp4', 'Stairs/32.mp4', 'Stairs/320.mp4', 'Stairs/322.mp4', 'Stairs/325.mp4', 'Stairs/326.mp4', 'Stairs/327.mp4', 'Stairs/328.mp4', 'Stairs/329.mp4', 'Stairs/33.mp4', 'Stairs/330.mp4', 'Stairs/331.mp4', 'Stairs/334.mp4', 'Stairs/335.mp4', 'Stairs/336.mp4', 'Stairs/337.mp4', 'Stairs/340.mp4', 'Stairs/342.mp4', 'Stairs/343.mp4', 'Stairs/344.mp4', 'Stairs/345.mp4', 'Stairs/349.mp4', 'Stairs/35.mp4', 'Stairs/350.mp4', 'Stairs/352.mp4', 'Stairs/353.mp4', 'Stairs/354.mp4', 'Stairs/356.mp4', 'Stairs/358.mp4', 'Stairs/36.mp4', 'Stairs/360.mp4', 'Stairs/364.mp4', 'Stairs/365.mp4', 'Stairs/366.mp4', 'Stairs/367.mp4', 'Stairs/368.mp4', 'Stairs/369.mp4', 'Stairs/37.mp4', 'Stairs/371.mp4', 'Stairs/372.mp4', 'Stairs/373.mp4', 'Stairs/374.mp4', 'Stairs/375.mp4', 'Stairs/376.mp4', 'Stairs/377.mp4', 'Stairs/378.mp4', 'Stairs/379.mp4', 'Stairs/38.mp4', 'Stairs/380.mp4', 'Stairs/381.mp4', 'Stairs/383.mp4', 'Stairs/387.mp4', 'Stairs/388.mp4', 'Stairs/389.mp4', 'Stairs/390.mp4', 'Stairs/392.mp4', 'Stairs/394.mp4', 'Stairs/395.mp4', 'Stairs/396.mp4', 'Stairs/399.mp4', 'Stairs/4.mp4', 'Stairs/40.mp4', 'Stairs/400.mp4', 'Stairs/401.mp4', 'Stairs/41.mp4', 'Stairs/42.mp4', 'Stairs/45.mp4', 'Stairs/46.mp4', 'Stairs/47.mp4', 'Stairs/48.mp4', 'Stairs/49.mp4', 'Stairs/5.mp4', 'Stairs/50.mp4', 'Stairs/51.mp4', 'Stairs/52.mp4', 'Stairs/53.mp4', 'Stairs/54.mp4', 'Stairs/57.mp4', 'Stairs/58.mp4', 'Stairs/59.mp4', 'Stairs/6.mp4', 'Stairs/60.mp4', 'Stairs/61.mp4', 'Stairs/62.mp4', 'Stairs/63.mp4', 'Stairs/64.mp4', 'Stairs/65.mp4', 'Stairs/66.mp4', 'Stairs/67.mp4', 'Stairs/68.mp4', 'Stairs/69.mp4', 'Stairs/7.mp4', 'Stairs/70.mp4', 'Stairs/71.mp4', 'Stairs/72.mp4', 'Stairs/73.mp4', 'Stairs/74.mp4', 'Stairs/75.mp4', 'Stairs/78.mp4', 'Stairs/79.mp4', 'Stairs/8.mp4', 'Stairs/80.mp4', 'Stairs/81.mp4', 'Stairs/82.mp4', 'Stairs/83.mp4', 'Stairs/84.mp4', 'Stairs/85.mp4', 'Stairs/86.mp4', 'Stairs/87.mp4', 'Stairs/88.mp4', 'Stairs/89.mp4', 'Stairs/90.mp4', 'Stairs/91.mp4', 'Stairs/92.mp4', 'Stairs/93.mp4', 'Stairs/95.mp4', 'Stairs/96.mp4', 'Stairs/98.mp4', 'Stairs/99.mp4']\n"
     ]
    }
   ],
   "source": [
    "print(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doors/107.mp4', 'Doors/108.mp4', 'Doors/115.mp4', 'Doors/123.mp4', 'Doors/127.mp4', 'Doors/13.mp4', 'Doors/134.mp4', 'Doors/138.mp4', 'Doors/139.mp4', 'Doors/149.mp4', 'Doors/150.mp4', 'Doors/151.mp4', 'Doors/152.mp4', 'Doors/158.mp4', 'Doors/164.mp4', 'Doors/171.mp4', 'Doors/175.mp4', 'Doors/18.mp4', 'Doors/180.mp4', 'Doors/182.mp4', 'Doors/184.mp4', 'Doors/189.mp4', 'Doors/191.mp4', 'Doors/193.mp4', 'Doors/197.mp4', 'Doors/198.mp4', 'Doors/20.mp4', 'Doors/203.mp4', 'Doors/210.mp4', 'Doors/212.mp4', 'Doors/215.mp4', 'Doors/221.mp4', 'Doors/224.mp4', 'Doors/231.mp4', 'Doors/245.mp4', 'Doors/254.mp4', 'Doors/256.mp4', 'Doors/261.mp4', 'Doors/264.mp4', 'Doors/267.mp4', 'Doors/268.mp4', 'Doors/271.mp4', 'Doors/272.mp4', 'Doors/274.mp4', 'Doors/278.mp4', 'Doors/281.mp4', 'Doors/284.mp4', 'Doors/285.mp4', 'Doors/286.mp4', 'Doors/29.mp4', 'Doors/292.mp4', 'Doors/302.mp4', 'Doors/304.mp4', 'Doors/305.mp4', 'Doors/313.mp4', 'Doors/314.mp4', 'Doors/326.mp4', 'Doors/330.mp4', 'Doors/333.mp4', 'Doors/335.mp4', 'Doors/336.mp4', 'Doors/339.mp4', 'Doors/343.mp4', 'Doors/344.mp4', 'Doors/351.mp4', 'Doors/353.mp4', 'Doors/355.mp4', 'Doors/357.mp4', 'Doors/358.mp4', 'Doors/36.mp4', 'Doors/375.mp4', 'Doors/379.mp4', 'Doors/386.mp4', 'Doors/389.mp4', 'Doors/391.mp4', 'Doors/392.mp4', 'Doors/47.mp4', 'Doors/5.mp4', 'Doors/51.mp4', 'Doors/53.mp4', 'Doors/54.mp4', 'Doors/60.mp4', 'Doors/62.mp4', 'Doors/63.mp4', 'Doors/64.mp4', 'Doors/7.mp4', 'Doors/71.mp4', 'Doors/74.mp4', 'Doors/76.mp4', 'Doors/81.mp4', 'Doors/83.mp4', 'Doors/86.mp4', 'Doors/92.mp4', 'Doors/93.mp4', 'Stairs/1.mp4', 'Stairs/108.mp4', 'Stairs/109.mp4', 'Stairs/114.mp4', 'Stairs/119.mp4', 'Stairs/121.mp4', 'Stairs/130.mp4', 'Stairs/135.mp4', 'Stairs/145.mp4', 'Stairs/151.mp4', 'Stairs/152.mp4', 'Stairs/154.mp4', 'Stairs/155.mp4', 'Stairs/156.mp4', 'Stairs/158.mp4', 'Stairs/16.mp4', 'Stairs/164.mp4', 'Stairs/167.mp4', 'Stairs/168.mp4', 'Stairs/181.mp4', 'Stairs/187.mp4', 'Stairs/192.mp4', 'Stairs/196.mp4', 'Stairs/197.mp4', 'Stairs/2.mp4', 'Stairs/201.mp4', 'Stairs/202.mp4', 'Stairs/204.mp4', 'Stairs/205.mp4', 'Stairs/208.mp4', 'Stairs/211.mp4', 'Stairs/222.mp4', 'Stairs/223.mp4', 'Stairs/225.mp4', 'Stairs/226.mp4', 'Stairs/228.mp4', 'Stairs/23.mp4', 'Stairs/235.mp4', 'Stairs/237.mp4', 'Stairs/238.mp4', 'Stairs/239.mp4', 'Stairs/241.mp4', 'Stairs/243.mp4', 'Stairs/251.mp4', 'Stairs/258.mp4', 'Stairs/263.mp4', 'Stairs/271.mp4', 'Stairs/275.mp4', 'Stairs/284.mp4', 'Stairs/287.mp4', 'Stairs/294.mp4', 'Stairs/296.mp4', 'Stairs/298.mp4', 'Stairs/302.mp4', 'Stairs/310.mp4', 'Stairs/316.mp4', 'Stairs/321.mp4', 'Stairs/323.mp4', 'Stairs/324.mp4', 'Stairs/332.mp4', 'Stairs/333.mp4', 'Stairs/338.mp4', 'Stairs/339.mp4', 'Stairs/34.mp4', 'Stairs/341.mp4', 'Stairs/346.mp4', 'Stairs/347.mp4', 'Stairs/348.mp4', 'Stairs/351.mp4', 'Stairs/355.mp4', 'Stairs/357.mp4', 'Stairs/359.mp4', 'Stairs/361.mp4', 'Stairs/362.mp4', 'Stairs/363.mp4', 'Stairs/370.mp4', 'Stairs/382.mp4', 'Stairs/384.mp4', 'Stairs/385.mp4', 'Stairs/386.mp4', 'Stairs/39.mp4', 'Stairs/391.mp4', 'Stairs/393.mp4', 'Stairs/397.mp4', 'Stairs/398.mp4', 'Stairs/43.mp4', 'Stairs/44.mp4', 'Stairs/55.mp4', 'Stairs/76.mp4', 'Stairs/77.mp4', 'Stairs/9.mp4', 'Stairs/94.mp4', 'Stairs/97.mp4']\n"
     ]
    }
   ],
   "source": [
    "print(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: \n",
    "Setup the train_dataset and valid_dataset (validation/testing).   Here we setting up training batch sets of 16.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tesnorflow 2.*\n",
    "train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),\n",
    "                output_types=(tf.float32, tf.int16),\n",
    "                output_shapes=((SEQUENCE_LENGTH, 1280), (len(LABELS))))\n",
    "                 \n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),\n",
    "                 output_types=(tf.float32, tf.int16),\n",
    "                 output_shapes=((SEQUENCE_LENGTH, 1280), (len(LABELS))))\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((16, 40, 1280), (16, 2)), types: (tf.float32, tf.int16)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((16, 40, 1280), (16, 2)), types: (tf.float32, tf.int16)>\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mylog directory = /Users/subhi/Downloads/demo/Data1/train_log\n"
     ]
    }
   ],
   "source": [
    "BASE_DATA_PATH = '/Users/subhi/Downloads/demo/Data1'\n",
    "mylog_dir = os.path.join( BASE_DATA_PATH, \"train_log\")\n",
    "print(\"Mylog directory = \" + mylog_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 0.3520 - accuracy: 0.8487 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_top_k_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "38/38 [==============================] - 29s 753ms/step - loss: 0.0772 - accuracy: 0.9720 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9830 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0399 - accuracy: 0.9786 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9659 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "38/38 [==============================] - 32s 843ms/step - loss: 0.0408 - accuracy: 0.9885 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.0123 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9886 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 0.0540 - accuracy: 0.9901 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "38/38 [==============================] - 31s 811ms/step - loss: 0.0307 - accuracy: 0.9885 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "38/38 [==============================] - 31s 809ms/step - loss: 0.0430 - accuracy: 0.9901 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "38/38 [==============================] - 31s 811ms/step - loss: 0.0218 - accuracy: 0.9934 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.0129 - accuracy: 0.9951 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 2.2699e-04 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "38/38 [==============================] - 32s 841ms/step - loss: 0.0319 - accuracy: 0.9901 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0319 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "38/38 [==============================] - 31s 805ms/step - loss: 0.0216 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0394 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.0217 - accuracy: 0.9951 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.1026 - accuracy: 0.9918 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.0163 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.0124 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "38/38 [==============================] - 35s 933ms/step - loss: 0.0235 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 0.9886 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "38/38 [==============================] - 36s 954ms/step - loss: 0.0174 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 2.9727e-05 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "38/38 [==============================] - 35s 927ms/step - loss: 0.0123 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 0.9886 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 0.0358 - accuracy: 0.9934 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 0.0175 - accuracy: 0.9951 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9773 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 0.0137 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 3.1698e-07 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "38/38 [==============================] - 34s 898ms/step - loss: 0.0307 - accuracy: 0.9918 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 0.0184 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0218 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.0054 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0534 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "38/38 [==============================] - 32s 829ms/step - loss: 0.0096 - accuracy: 0.9951 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "38/38 [==============================] - 33s 865ms/step - loss: 0.0050 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 0.9886 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 0.0112 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0268 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "38/38 [==============================] - 39s 1s/step - loss: 0.0126 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 1.9215e-04 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "38/38 [==============================] - 39s 1s/step - loss: 3.7814e-04 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.2087e-06 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 0.0242 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0322 - val_accuracy: 0.9886 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0626 - accuracy: 0.9934 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9886 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "38/38 [==============================] - 30s 784ms/step - loss: 0.0198 - accuracy: 0.9934 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0325 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "38/38 [==============================] - 30s 786ms/step - loss: 3.7081e-04 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "38/38 [==============================] - 30s 785ms/step - loss: 0.0091 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0572 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "38/38 [==============================] - 28s 748ms/step - loss: 0.0032 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.0025 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 0.0202 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "38/38 [==============================] - 35s 912ms/step - loss: 0.0013 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "38/38 [==============================] - 30s 799ms/step - loss: 0.0052 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "38/38 [==============================] - 28s 743ms/step - loss: 0.0234 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "38/38 [==============================] - 39s 1s/step - loss: 0.0175 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "38/38 [==============================] - 32s 841ms/step - loss: 0.0091 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "38/38 [==============================] - 30s 797ms/step - loss: 1.6051e-07 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 4.9480e-04 - val_accuracy: 1.0000 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "38/38 [==============================] - 38s 1s/step - loss: 4.0119e-04 - accuracy: 1.0000 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "38/38 [==============================] - 37s 975ms/step - loss: 0.0113 - accuracy: 0.9984 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0879 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 0.0178 - accuracy: 0.9967 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0352 - val_accuracy: 0.9943 - val_top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a36d5a9e8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(os.path.join('tmp'), update_freq=1000)\n",
    "model.fit(train_dataset, epochs=50, callbacks=[tensorboard_callback], validation_data=valid_dataset)\n",
    "#model.fit(train_dataset, epochs=50,validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11:  save the tensorflow model to an h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.file=os.path.join(BASE_PATH,'my_model.h5')\n",
    "model.save(model.file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 13: try to conver the model to tflite --- Support to come 2019 (when?)--Curently LSTM conversion to TFLite NOT supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 14: run evaluation on the test data feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 15: Run predictions on the test data feature extracted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
