{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subhi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import glob\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the constants\n",
    "IMG_SHAPE  = 224  # size of our input image needed for our model IMG_SHAPE x IMG_SHAPE x 3 (color)\n",
    "BASE_DIRECTORY ='/Users/subhi/Downloads/demo/Data1/'\n",
    "\n",
    "# labels for the 3 classes\n",
    "LABELS = ['Stairs','Doors']\n",
    "\n",
    "DECISION_DIFFERENCE_THRESHOLD = 0.1\n",
    "SEQUENCE_LENGTH = 40\n",
    "FEATURE_LENGTH = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load a model from an h5 file\n",
    "def loadModelFrom_H5_File(model_file):\n",
    "    new_model = tf.keras.models.load_model(model_file)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a feature extraction model (MobileNetV2 CNN)\n",
    "def CreateCNN(image_shape):\n",
    "    mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(image_shape,image_shape,3), \n",
    "                                                                  include_top=False, weights='imagenet')\n",
    "    cnn_output = mobilenet_v2.output\n",
    "    pooling_output = tf.keras.layers.GlobalAveragePooling2D()(cnn_output)\n",
    "    feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)\n",
    "    \n",
    "    return feature_extraction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to re-size and pre-process an image from the camera and return a tensor\n",
    "def ProcessImage(image,image_size):\n",
    "    img = tf.image.resize(image, (image_size,image_size))\n",
    "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)          \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all of the features from a list of properly sized images\n",
    "def ExtractFeatures(image_array,feature_extraction_model):\n",
    "    all_features = []\n",
    "    image_array = np.asarray(image_array)\n",
    "    \n",
    "    # loop through all the images in the image_array list and extract their features\n",
    "    for i in range(image_array.shape[0]):\n",
    "        features = feature_extraction_model(image_array[i])\n",
    "        features = tf.reshape(features,(features.shape[0], -1))\n",
    "        features = features.numpy()\n",
    "        all_features.append(features)\n",
    "    \n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes a list of feature vectors and returns a properly formatted numpy array for prediction\n",
    "def PrepareFeatures(features,sequence_length,feature_size):\n",
    "    features = np.asarray(features)\n",
    "    features = np.reshape(features,(1,sequence_length,feature_size))\n",
    "    padded_sequence = np.zeros((1,sequence_length,feature_size))\n",
    "    padded_sequence[0:feature_size] = np.array(features)\n",
    "    \n",
    "    return padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to use an LSTM model to make a prediction on live video data\n",
    "def predict(input, model):\n",
    "    prediction = model.predict(input, batch_size=1, verbose=0)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make a classification decision and return a label\n",
    "def makeDecision(predictions, class_labels):\n",
    "    max_index = np.argmax(predictions)\n",
    "    predictions2 = np.delete(predictions,[max_index],None)\n",
    "    max_index2 = np.argmax(predictions2)\n",
    "    \n",
    "    if(max_index2 >= max_index):\n",
    "        max_index2 = max_index2 + 1\n",
    "        \n",
    "    if(predictions[0][max_index] >= predictions[0][max_index2] and \n",
    "      (predictions[0][max_index]-predictions[0][max_index2]) > DECISION_DIFFERENCE_THRESHOLD): \n",
    "        label = class_labels[max_index]\n",
    "    else:\n",
    "        label = \"Unknown\"\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a folder to store prediction info in the specified parent directory\n",
    "def CreatePredictionFolder(parent_dir,classification):\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    prediction_directory = classification + '_live_' + timestr\n",
    "    prediction_directory = os.path.join(parent_dir,prediction_directory)\n",
    "    os.mkdir(prediction_directory)\n",
    "    \n",
    "    return prediction_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes a list of BGR frames and saves them to the prediction_folder\n",
    "# the frames must be in BGR format as OpenCV will rearrange the channels during saving\n",
    "# so that each saved frame will be in RGB format\n",
    "def SavePredictionFrames(all_BGR_frames,prediction_folder):    \n",
    "    all_frames = np.asarray(all_BGR_frames)\n",
    "    for i in range(all_frames.shape[0]):\n",
    "        img_file = 'Image_' + str(i+1) + '.jpg'\n",
    "        img_file = os.path.join(prediction_folder,img_file)\n",
    "        frame = all_frames[i]\n",
    "        cv2.imwrite(img_file,frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a .txt file for all of the feature vectors used to make a prediction\n",
    "def SaveVideoFeatures(prediction_dir,features):\n",
    "    filename = os.path.join(prediction_dir,'FeatureVectors.txt')\n",
    "    np.savetxt(filename,features[0],delimiter = ', ',newline = '\\n\\n\\n',\n",
    "               header = '\\n\\n', footer = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a .txt file for all of the prediction results (label and prediction vector)\n",
    "def SavePredictionResults(prediction_dir,prediction_results,classification):\n",
    "    filename = os.path.join(prediction_dir,'PredictionResults.txt')\n",
    "    np.savetxt(filename,prediction_results,delimiter = ', ',newline = '\\n\\n',\n",
    "               header = '\\n\\nPrediction: ' + classification + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1202 12:00:22.234798 4603919680 hdf5_format.py:203] Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# load the saved model and create the MobileNetV2 CNN\n",
    "model_files = os.path.join(BASE_DIRECTORY, '*.h5')\n",
    "model_paths = tf.io.gfile.glob(model_files)\n",
    "model_file = model_paths[0]\n",
    "model = loadModelFrom_H5_File(model_file)   \n",
    "mobilenet = CreateCNN(IMG_SHAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a folder to store all of the live capture predictions if it doesn't already exist\n",
    "results_directory = os.path.join(BASE_DIRECTORY,'LiveCaptureResults')\n",
    "if not os.path.exists(results_directory):\n",
    "    os.mkdir(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-884d56db85c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# capture a frame from the video capture stream and adjust its size and channel format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mis_capturing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframeorig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mBGR_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeorig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMG_SHAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_LINEAR\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# resize BGR frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mRGB_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBGR_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# convert BGR to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.1) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "url = 'http://10.0.0.48:8081/'\n",
    "vc = cv2.VideoCapture(url)\n",
    "#vc = cv2.VideoCapture(0)  #opencv handler for capturing video\n",
    "current_frame = 0\n",
    "max_images = SEQUENCE_LENGTH\n",
    "all_images = []\n",
    "all_BGR_frames = []\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    is_capturing, frame = vc.read()    #read frame\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored \n",
    "    \n",
    "else:\n",
    "    is_capturing = False\n",
    "\n",
    "while is_capturing:\n",
    "    try:    \n",
    "        \n",
    "        # capture a frame from the video capture stream and adjust its size and channel format        \n",
    "        is_capturing, frameorig = vc.read()\n",
    "        BGR_frame = cv2.resize(frameorig,(IMG_SHAPE,IMG_SHAPE),interpolation = cv2.INTER_LINEAR)  # resize BGR frame\n",
    "        RGB_frame = cv2.cvtColor(BGR_frame,cv2.COLOR_BGR2RGB)    # convert BGR to RGB\n",
    "        \n",
    "        all_BGR_frames.append(BGR_frame)\n",
    "        \n",
    "        # pre-process image into a tensor and convert the underlying data from type uint8 to float32\n",
    "        image = ProcessImage(RGB_frame,IMG_SHAPE)\n",
    "        all_images.append(image)\n",
    "        max_images -= 1           \n",
    "        \n",
    "        # after getting all SEQUENCE_LENGTH frames\n",
    "        if max_images == 0:           \n",
    "            all_features = ExtractFeatures(all_images,mobilenet)\n",
    "            prediction_input = PrepareFeatures(all_features,SEQUENCE_LENGTH,FEATURE_LENGTH)\n",
    "            prediction = predict(prediction_input, model)\n",
    "            classification = makeDecision(prediction,LABELS)\n",
    "            \n",
    "            # make folder to store each separate prediction\n",
    "            prediction_directory = CreatePredictionFolder(results_directory,classification)\n",
    "            \n",
    "            # create file to store the prediction results and the feature vectors \n",
    "            SaveVideoFeatures(prediction_directory,prediction_input)\n",
    "            SavePredictionResults(prediction_directory,prediction,classification)\n",
    "            \n",
    "            # save all of the SEQUENCE_LENGTH frames used to make the prediction\n",
    "            SavePredictionFrames(all_BGR_frames,prediction_directory)          \n",
    "            current_frame = 0 \n",
    "            all_images = []\n",
    "            all_BGR_frames = []\n",
    "            max_images = SEQUENCE_LENGTH\n",
    "            \n",
    "        current_frame += 1\n",
    "                               \n",
    "    except KeyboardInterrupt:\n",
    "        vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
